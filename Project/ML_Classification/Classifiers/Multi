from google.colab import drive
drive.mount('/content/drive')
dataset = spark.read.csv('drive/My Drive/UNSW-NB15.csv',inferSchema=True, header =True)\
.toDF("srcip", "sport", "dstip", "dsport", "proto", "state", "dur", "sbytes", "dbytes", "sttl", "dttl",
               "sloss", "dloss", "service", "Sload", "Dload", "Spkts", "Dpkts", "swin", "dwin", "stcpb", "dtcpb",
               "smeansz", "dmeansz", "trans_depth", "res_bdy_len", "Sjit", "Djit", "Stime", "Ltime", "Sintpkt", "Dintpkt",
               "tcprtt", "synack", "ackdat", "is_sm_ips_ports", "ct_state_ttl", "ct_flw_http_mthd", "is_ftp_login", "ct_ftp_cmd",
               "ct_srv_src", "ct_srv_dst", "ct_dst_ltm", "ct_src_ ltm", "ct_src_dport_ltm", "ct_dst_sport_ltm", "ct_dst_src_ltm",
               "attack_cat", "Label")

#Showing Data with column Header
dataset.show()

#Dropping Label column
dataset=dataset.drop("Label")
dataset.show()
dataset=dataset.na.fill(value="normal", subset=["attack_cat"])
dataset.show()
dataset.select('attack_cat').distinct().show()
dataset=dataset["dloss", "dwin", "stcpb", "dtcpb","smeansz", "dmeansz", "trans_depth","attack_cat"]
dataset.printSchema()

from pyspark.ml.feature import StringIndexer
string_cols = ["attack_cat"]

for column in list(string_cols):
    stringIndexer = StringIndexer(inputCol=column, outputCol="indexed_"+column, handleInvalid="error")
    model = stringIndexer.fit(dataset)
    dataset = model.transform(dataset)

dataset=dataset.drop("attack_cat")
cols=dataset.columns
cols.remove("indexed_attack_cat")

# Let us import the vector assembler
from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=cols,outputCol="features")

# Now let us use the transform method to transform our dataset
dataset=assembler.transform(dataset)
dataset.select("features").show(truncate=False)
(trainingData, testData) = dataset.randomSplit([0.7, 0.3])

from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
dt = DecisionTreeClassifier(labelCol="indexed_attack_cat", featuresCol="features",impurity='entropy',maxBins=134, maxDepth=4,seed=1234)
model = dt.fit(trainingData)
predictions = model.transform(testData)
evaluator = MulticlassClassificationEvaluator(\
labelCol="indexed_attack_cat", predictionCol="prediction",\
metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Test accuracy =  " , accuracy)
print(model.toDebugString)

#RandomForest
from pyspark.ml.classification import RandomForestClassifier

#rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')
rf = RandomForestClassifier(featuresCol='features', labelCol='indexed_attack_cat',numTrees=15)
train, test = dataset.randomSplit([0.7, 0.3], seed = 2018)
rfModel = rf.fit(train)
predictions = rfModel.transform(test)
pr = predictions.toPandas()
evaluator = MulticlassClassificationEvaluator(\
labelCol="indexed_attack_cat", predictionCol="prediction",\
metricName="accuracy")
accuracy = evaluator.evaluate(predictions )
print("Test accuracy =  " , accuracy)